# Fine-Tuning Wav2Vec2 for Accented English ASR

This project demonstrates how to fine-tune Facebook's `wav2vec2-base-960h` model on accented English speech data (Common Voice) using the Hugging Face Transformers and Datasets libraries.

---

## ğŸ“ Project Structure

```
fine_tune_wav2vec2_accented/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ common_voice/       # Raw data directory
â”‚   â”œâ”€â”€ train/              # Metadata + WAV paths for training
â”‚   â”œâ”€â”€ val/                # Validation set
â”‚   â””â”€â”€ test/               # Testing set (for evaluation)
â”‚
â”œâ”€â”€ logs/                  # SLURM output logs
â”œâ”€â”€ models/
â”‚   â””â”€â”€ checkpoints/
â”‚       â””â”€â”€ checkpoint-3/   # Latest model checkpoint
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ fine_tuned_predictions.json  # Transcripts and references
â”‚   â”œâ”€â”€ leaderboard.csv              # Logged metrics
â”‚   â””â”€â”€ plots/
â”‚       â””â”€â”€ wer_cer_bar.png          # Bar plot of WER/CER
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ fine_tune.py                # Training script
â”‚   â”œâ”€â”€ evaluate_finetuned.py       # WER/CER + prediction generation
â”‚   â”œâ”€â”€ prepare_dataset.py          # Metadata formatting
â”‚   â””â”€â”€ plot_leaderboard.py         # Visualization script
â”‚
â”œâ”€â”€ run_finetune.sh                 # SLURM job script
â”œâ”€â”€ requirements.txt                # Conda/pip dependencies
â””â”€â”€ README.md                       # This file
```

---

## ğŸš€ Setup

### 1. Create and activate conda environment

```bash
conda create -n asr_env python=3.10 -y
conda activate asr_env
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸ¯ Fine-Tuning

To train the model from scratch on the `train/` and `val/` splits:

```bash
bash run_finetune.sh
```

Logs will be saved in `logs/`. Checkpoints are stored in `models/checkpoints/`.

---

## ğŸ“ˆ Evaluation

After training completes:

```bash
python scripts/evaluate_finetuned.py
```

This will:

* Run inference on `data/test/`
* Save predictions to `results/fine_tuned_predictions.json`
* Compute and print **WER** and **CER**
* Log metrics to `leaderboard.csv`

---

## ğŸ“Š Visualization

To visualize evaluation metrics as a bar plot:

```bash
python scripts/plot_leaderboard.py
```

Output: `results/plots/wer_cer_bar.png`

---

## âœ… Outputs 

```
WER: 1.0278
CER: 0.8531 
```

![WER/CER Plot](results/plots/wer_cer_bar.png)

---

## ğŸ“Œ Notes

* Make sure all audio is 16kHz mono.
* Use `prepare_dataset.py` to convert raw Common Voice TSVs into metadata.
* Models and processor are loaded from `facebook/wav2vec2-base-960h`.

---

## ğŸ“¬ Contact

For any questions, feel free to reach out or open an issue.

Happy fine-tuning! ğŸ§
